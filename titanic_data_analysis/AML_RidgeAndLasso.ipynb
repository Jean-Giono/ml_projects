{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the libraries needed for the calculation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    \"\"\" Predict our h(x) in function of their features \n",
    "      INPUT :\n",
    "        X shape(n, m)\n",
    "        W shape(n, 1)\n",
    "      OUTPUT :\n",
    "        h shape(1, m)\"\"\"\n",
    "  \n",
    "    return np.dot(W.T, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeCostRidge(X, W, Y, lamda):\n",
    "    \"\"\" Compute cost for ridge regression \n",
    "      INPUT :\n",
    "        X      shape(n, m)\n",
    "        W      shape(n, 1)\n",
    "        Y      shape(1, m)\n",
    "        lamda  float\n",
    "      OUTPUT :\n",
    "        J float\"\"\"\n",
    "    \n",
    "    _, m = X.shape\n",
    "        \n",
    "    J = 0\n",
    "    #Jridge = (lamda / (2*m)) * np.sum(np.square(W))\n",
    "    #J = 1/(2*m) * np.dot((predict(X, W)-Y), (predict(X, W)-Y).T) + Jridge\n",
    "        \n",
    "    # predict(W, W) = np.dot(W.T, W)\n",
    "    J = 1/(2*m) * (np.dot((predict(X, W)-Y), (predict(X, W)-Y).T) + lamda * predict(W, W))\n",
    "        \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentRidge(X, W, Y, alpha, num_iters, lamda):\n",
    "    \"\"\" gradientDescent compute gradient descent to learn best W\n",
    "      INPUT :\n",
    "        X           shape(n, m)\n",
    "        W           shape(n, 1)\n",
    "        Y           shape(1, m)\n",
    "        alpha       float\n",
    "        num_iters   integer\n",
    "        lamda       float\n",
    "      OUTPUT :\n",
    "        W           shape(n, 1)\n",
    "        J_history   shape(num_iters,)\"\"\"\n",
    "    \n",
    "    _, m = X.shape\n",
    "      \n",
    "    J_history = np.zeros(num_iters)\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        J_history[i] = ComputeCostRidge(X, W, Y, lamda)\n",
    "            \n",
    "        gradient = (alpha/m) * (np.dot(X, (predict(X, W)-Y).T) + lamda * W)\n",
    "        W = W - gradient\n",
    "            \n",
    "    return W, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of the dataset\n",
    "multivariate_regression = np.genfromtxt('multivariate_regression.txt', delimiter=',')\n",
    "\n",
    "m, _ = multivariate_regression.shape\n",
    "\n",
    "#X = np.stack(multivariate_regression[:, :2], axis=1)\n",
    "X = multivariate_regression[:, :2] \n",
    "Y = multivariate_regression[:, 2].reshape((1, m))\n",
    "\n",
    "# Standardisation of the features\n",
    "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "X_1 = np.stack(np.hstack((np.ones(m).reshape(m, 1), X)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With W = [1 1 1]\n",
      "Cost Function = 65591047226.09408\n"
     ]
    }
   ],
   "source": [
    "W = np.array((1,1,1)).reshape(3, 1)\n",
    "\n",
    "# same results as the cost function without regularization when lamda = 0 \n",
    "#print(\"With W = \"+str(np.squeeze(W))+\"\\nCost Function = \" + str(np.squeeze(ComputeCostRidge(X_1, W, Y, lamda=0))))\n",
    "\n",
    "print(\"With W = \"+str(np.squeeze(W))+\"\\nCost Function = \" + str(np.squeeze(ComputeCostRidge(X_1, W, Y, lamda=100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of the parameters W after 200 iterations of gradient descent :\n",
      "[[292660.65750329]\n",
      " [ 82623.98092499]\n",
      " [ 15240.47187713]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array((0, 0, 0)).reshape((3,1))\n",
    "alpha = 0.01\n",
    "iterations = 200\n",
    "\n",
    "# same results as the gradient descent without regularization when lamda = 0\n",
    "lamda = 0.5\n",
    "\n",
    "\n",
    "W, J_history = gradientDescentRidge(X_1, W, Y, alpha, iterations, lamda)\n",
    "print(\"Values of the parameters W after \" + str(iterations) + \" iterations of gradient descent :\\n\" + str(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\" Apply sigmoid function to a number\n",
    "      INPUT :\n",
    "        z shape(1, m)\n",
    "      OUTPUT :\n",
    "        g(z) shape(1, m)\"\"\"\n",
    "  \n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLasso(X, W):\n",
    "    \"\"\" This function makes predictions by linearly combining W and X. \n",
    "     Sigmoid is apply to born the output between [0, 1]\n",
    "     INPUT : \n",
    "        X shape(n, m)\n",
    "        W shape(n, 1)\n",
    "     OUTPUT :  \n",
    "        h shape(1, m)\"\"\"\n",
    "  \n",
    "    return sigmoid(np.dot(W.T, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCostLasso(X, W, Y, lamda) :\n",
    "    \"\"\" Compute the cost for the W parameters (Lasso L2)\n",
    "      INPUT : \n",
    "        X       shape(n, m)\n",
    "        W       shape(n, 1)\n",
    "        Y       shape(1, m)\n",
    "        lamda   float\n",
    "      OUTPUT : \n",
    "        J       float\"\"\"\n",
    "        \n",
    "    Jlasso = (lamda / (2*m)) * np.sum(np.square(W))\n",
    "    J = -(1/m) * np.sum(Y * np.log(predictLasso(X, W))+ (1-Y) * np.log(1-predictLasso(X, W)))\n",
    "    \n",
    "    return J + Jlasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescentLasso(X, W, Y, alpha, num_iters, lamda):\n",
    "    \"\"\" gradientDescent updates W by taking num_iters\n",
    "      gradient steps with learning rate alpha \n",
    "      INPUT :\n",
    "        X         shape(n, m)\n",
    "        W         shape(n, 1)\n",
    "        Y         shape(1, m)\n",
    "        alpha     float\n",
    "        lamda     float\n",
    "        num_iters integer\n",
    "      OUTPUT :\n",
    "        W         shape(n, 1)\n",
    "        J_history shape(num_iters,)\"\"\"\n",
    "\n",
    "    _, m = X.shape\n",
    "\n",
    "    J_history = np.zeros(num_iters)\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        \n",
    "        J_history[i] = computeCostLasso(X, W, Y, lamda)\n",
    "        \n",
    "        gradient = (alpha/m) * (np.dot(X, (predictLasso(X, W)-Y).T) + lamda * W)\n",
    "        W = W - gradient\n",
    "        \n",
    "        \n",
    "    return W, J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = np.genfromtxt('logistic_regression.txt', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = logistic_regression[:, :2].shape\n",
    "X = logistic_regression[:, :2]\n",
    "X_1 = np.stack(np.hstack((np.ones(m).reshape(m, 1), X)), axis=1)\n",
    "Y = logistic_regression[:, 2].reshape(1, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.86892535e-01 1.04767902e-04 9.53426817e-02 9.95024445e-01\n",
      "  9.98968205e-01]]\n"
     ]
    }
   ],
   "source": [
    "W = np.array((-24, 0.2, 0.2)).reshape(3, 1)\n",
    "\n",
    "h_5 = predictLasso(X_1[:, :5], W)\n",
    "print(h_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With W = [-24.    0.2   0.2]\n",
      "Cost Function = 1.6585301938265977\n"
     ]
    }
   ],
   "source": [
    "W = np.array((-24, 0.2, 0.2)).reshape(3, 1)\n",
    "\n",
    "#J = computeCostLasso(X_1, W, Y, lamda=0)\n",
    "# same results as the cost function without regularization when lamda = 0 \n",
    "\n",
    "J = computeCostLasso(X_1, W, Y, lamda=0.5)\n",
    "\n",
    "print(\"With W = \"+str(np.squeeze(W))+\"\\nCost Function = \" +str(np.squeeze(J)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values of the parameters W after 20 iterations of gradient descent :\n",
      "[[-23.99769644]\n",
      " [  0.19570881]\n",
      " [  0.19331878]]\n",
      "Values of the cost function : 1.6436929607022337\n"
     ]
    }
   ],
   "source": [
    "W = np.array((-24, 0.2, 0.2)).reshape((3,1))\n",
    "alpha = 0.001\n",
    "iterations = 20\n",
    "lamda = 0.5\n",
    "\n",
    "# same results as the gradient descent function without regularization when lamda = 0\n",
    "\n",
    "W, J_history = gradientDescentLasso(X_1, W, Y, alpha, iterations, lamda)\n",
    "\n",
    "print(\"Values of the parameters W after \" + str(iterations) +  \\\n",
    "      \" iterations of gradient descent :\\n\" + str(W))\n",
    "print(\"Values of the cost function : \" +str(J_history[iterations-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
